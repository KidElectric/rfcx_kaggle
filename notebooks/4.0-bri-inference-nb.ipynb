{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai import learner\n",
    "from fastai.vision.all import *\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa as lb\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import matplotlib.patches as patch\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.io import wavfile\n",
    "from torch import cuda\n",
    "import shutil\n",
    "\n",
    "CC_ROOT=Path(\"D:\\\\KidElectric\\\\rfcx_kaggle\") #Cookiecutter datascience-style project\n",
    "CC_DATA_TEST=CC_ROOT.joinpath('data').joinpath('interim').joinpath('test')\n",
    "CC_MODEL_ROOT=CC_ROOT.joinpath('models').joinpath('fit')\n",
    "df_train = pd.DataFrame({\n",
    "    \"recording_id\": [path.stem for path in TRAIN_AUDIO_ROOT.glob(\"*.flac\")],\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"recording_id\": [path.stem for path in TEST_AUDIO_ROOT.glob(\"*.flac\")],\n",
    "})\n",
    "\n",
    "df_tp=pd.read_csv(CC_ROOT.joinpath('references').joinpath('train_tp.csv')).set_index('recording_id')\n",
    "\n",
    "df_fp=pd.read_csv(CC_ROOT.joinpath('references').joinpath('train_fp.csv')).set_index('recording_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a model:\n",
    "def which_singer(x): return x.parts[-1].split('_')[1] #Need to define this function first\n",
    "# model_name='rn101_50ep_512_clip_full_aug2_noflip_FB_120320_mixup0p5.pkl'\n",
    "model_name='rn101_50ep_lr8p32e-10_434_clip_TP_FP_NP_aug2_noflip_120420_mixup0p5.pkl'\n",
    "learn=load_learner(CC_MODEL_ROOT.joinpath(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Predict many images at once \n",
    "imgs=[png for png in CC_DATA_TEST.joinpath('clip_decomp').glob('*.png')]\n",
    "dl = learn.dls.test_dl(imgs, item_tfms=Resize(512,ResizeMethod.Squish),num_workers=0,bs=64)\n",
    "a,b=learn.get_preds(dl=dl) # or torch.softmax?\n",
    "prob_order= learn.dls.vocab\n",
    "recs=df_test['recording_id']\n",
    "df_prob=pd.DataFrame(index=df_test['recording_id'],columns=['s%d' % i for i in range(0,24,1)])\n",
    "m = nn.Softmax(dim=1)\n",
    "probs=m(a)\n",
    "for rec in recs:\n",
    "    temp=np.zeros((len(pngs),len(prob_order)))\n",
    "    n=0\n",
    "    for i,img in enumerate(imgs):\n",
    "        if rec == img.parts[-1].split('_')[0]:\n",
    "            temp[n,:]=probs[i,:]\n",
    "            n += 1\n",
    "    mp=np.mean(temp,axis=0) #Or should it be mean? -- doesn't seem to matter!\n",
    "    for i,spec in enumerate(prob_order):\n",
    "        if int(spec) < 24:\n",
    "            df_prob['s%s' % spec][rec]=float(mp[i])\n",
    "df_prob.to_csv(CC_MODEL_ROOT.parent.joinpath('predictions').joinpath('%s-sum.csv' % model_name[0:-4]))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_0.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_1.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_2.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_3.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_4.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_5.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_6.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_7.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model D:\\KidElectric\\rfcx_kaggle\\data\\processed\\resamp_15\\models\\rn50_50ep_224_resamp15_TP_FP_NP_aug2_noflip_8.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='192' class='' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      61.54% [192/312 14:32<09:05 0.1655]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict many images from many models:\n",
    "rs = 15\n",
    "nmodels=20\n",
    "path =CC_ROOT.joinpath('data').joinpath('processed').joinpath('resamp_%d' % rs)\n",
    "imgs=[png for png in CC_DATA_TEST.joinpath('clip_decomp').glob('*.png')]\n",
    "im_size=224\n",
    "nspec=24\n",
    "for ii in range(nmodels):\n",
    "    model_name='rn50_50ep_%d_resamp%d_TP_FP_NP_aug2_noflip_%d.pkl' % (im_size,rs,ii) #Each of these models was pretty bad on their own (~0.65 error rate)\n",
    "    mpath=path.joinpath('models')\n",
    "    print('Loading model %s' % mpath.joinpath(model_name))\n",
    "    learn=load_learner(mpath.joinpath(model_name))\n",
    "    dl = learn.dls.test_dl(imgs, item_tfms=Resize(im_size,ResizeMethod.Squish),num_workers=0,bs=64)\n",
    "    print('\\tPredicting...')\n",
    "    a,b=learn.get_preds(dl=dl) # or torch.softmax?\n",
    "    prob_order= learn.dls.vocab\n",
    "    recs=df_test['recording_id']\n",
    "    df_prob=pd.DataFrame(index=df_test['recording_id'],columns=['s%d' % i for i in range(0,24,1)])\n",
    "    m = nn.Softmax(dim=1)\n",
    "    probs=m(a)\n",
    "    for rec in recs:\n",
    "        temp=np.zeros((len(pngs),len(prob_order)))\n",
    "        n=0\n",
    "        for i,img in enumerate(imgs):\n",
    "            if rec == img.parts[-1].split('_')[0]:\n",
    "                temp[n,:]=probs[i,:]\n",
    "                n += 1\n",
    "        mp=np.mean(temp,axis=0) #Or should it be mean? -- doesn't seem to matter!\n",
    "        for i,spec in enumerate(prob_order):\n",
    "            if int(spec) < nspec:\n",
    "                df_prob['s%s' % spec][rec]=float(mp[i])\n",
    "    print('\\tSaving')\n",
    "    df_prob.to_csv(mpath.joinpath('%s.csv' % model_name[0:-4]))\n",
    "    \n",
    "#Load in multiple .csv and take the mean:\n",
    "csv_fns=mpath.glob( model_name[0:-7] + '*.csv')\n",
    "df_load={}\n",
    "for i,fn in enumerate(csv_fns):\n",
    "    df_load[i]= pd.read_csv(fn)\n",
    "b=df_load[0]\n",
    "for col in df_load[0].columns:\n",
    "    if col != 'recording_id':\n",
    "        for k in df_load.keys():\n",
    "            b[col]=b[col] + df_load[k][col]\n",
    "        b[col]= b[col]/nspec\n",
    "b.head()\n",
    "b.to_csv(mpath.joinpath('%s_%d_averaged.csv' % (model_name[0:-4],len(df_load))), index=False)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
